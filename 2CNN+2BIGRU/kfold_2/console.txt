>>> Imports:
#coding=utf-8

from __future__ import print_function

try:
    import numpy as np
except:
    pass

try:
    from utils import *
except:
    pass

try:
    import math
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    from keras.backend.tensorflow_backend import set_session
except:
    pass

try:
    from keras.models import Sequential, Model
except:
    pass

try:
    from keras.layers import Dense, Dropout, Activation, GRU
except:
    pass

try:
    from keras.layers import Embedding, concatenate, Flatten
except:
    pass

try:
    from keras.layers import Conv1D
except:
    pass

try:
    from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D, MaxPooling1D
except:
    pass

try:
    from keras.layers import Input
except:
    pass

try:
    from keras.layers.normalization import BatchNormalization
except:
    pass

try:
    import logging
except:
    pass

try:
    import json
except:
    pass

try:
    from keras.preprocessing import sequence
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint
except:
    pass

try:
    from hyperas.distributions import uniform, choice
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    import keras
except:
    pass

try:
    from keras import regularizers
except:
    pass

try:
    from keras.utils import multi_gpu_model
except:
    pass

try:
    from tensorflow.contrib.metrics import streaming_auc
except:
    pass

try:
    from sklearn.metrics import matthews_corrcoef, recall_score, f1_score
except:
    pass

try:
    from my_utils import *
except:
    pass

try:
    import os
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    from keras.backend.tensorflow_backend import set_session
except:
    pass

try:
    from adabound import AdaBound
except:
    pass

try:
    from sklearn.preprocessing import LabelBinarizer
except:
    pass

try:
    from keras.utils import to_categorical
except:
    pass

try:
    import sys
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
    }

>>> Data
   1: 
   2: max_sequence_size=800
   3: logging.debug("Target Gene Ontology :BP ")
   4: train_file = '/home/jiajunh/kfold_2/all_train.pkl'
   5: pssm_file = '/home/wanglei/data/Oxygen/npy'
   6: test_file = '/home/jiajunh/kfold_2/all_test.pkl'
   7: pssm_test_file = '/home/wanglei/data/Oxygen/npy'
   8: logging.info("Preparing train data ... ")
   9: 
  10: features = []
  11: train_df=pd.read_pickle(train_file)
  12: 
  13: def reshape(values):
  14:     values=np.hstack(values).reshape(len(values),len(values[0]))
  15:     return values
  16: 
  17: for i,row in train_df.iterrows():
  18:     pdb_id=row['pdb_id']
  19:     extra_feature=get_extra_feature(pssm_file,pdb_id)
  20:     features.append(extra_feature)
  21: 
  22: logging.info('Padding sequences to %d ... ' % max_sequence_size)
  23: X=sequence.pad_sequences(train_df['ngrams'].values,maxlen=max_sequence_size)
  24: y=train_df['labels'].values
  25: lb = LabelBinarizer()
  26: 
  27: y = lb.fit_transform(y)  # transfer label to binary value
  28: 
  29: #y = to_categorical(y)  # transfer binary label to one-hot. IMPORTANT
  30: X_all=np.array(X)
  31: y_all=np.array(y)
  32: pssm_ssacc=np.array(features)
  33: logging.info("Input shape: %s" % str(X_all.shape))
  34: logging.info("Output shape: %s" % str(y_all.shape))
  35: logging.info("pssm and ssacc shape:%s" % str(pssm_ssacc.shape))
  36: n = X_all.shape[0]
  37: 
  38: # randomize to shuffle first
  39: randomize = np.arange(n)
  40: np.random.shuffle(randomize)
  41: 
  42: X_train = X_all[randomize]
  43: y_train_temp = y_all[randomize]
  44: y_train=reshape(y_train_temp)
  45: X_extra = pssm_ssacc[randomize]
  46: 
  47: logging.info('------------------------------------------------')
  48: logging.info('preparing the test data...')
  49: 
  50: features_test = []
  51: test_df = pd.read_pickle(test_file)
  52: for i,row in test_df.iterrows():
  53:     pdb_id = row['pdb_id']
  54:     extra_feature = get_extra_feature(pssm_test_file, pdb_id)
  55:     features_test.append(extra_feature)
  56: logging.info('Padding sequences to %d ... ' % max_sequence_size)
  57: X_temp = sequence.pad_sequences(test_df['ngrams'].values, maxlen=max_sequence_size)
  58: y_temp = test_df['labels'].values
  59: X_test=np.array(X_temp)
  60: y_test_temp=np.array(y_temp)
  61: y_test=reshape(y_test_temp)
  62: pssm_ssacc_test=np.array(features_test)
  63: logging.info("Input shape: %s" % str(X_test.shape))
  64: logging.info("Output shape: %s" % str(y_test.shape))
  65: logging.info("pssm and ssacc shape:%s" % str(pssm_ssacc_test.shape))
  66: 
  67: 
  68: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     # num_amino_acids = 22  # alphabet of proteins +1 for padding
   4:     model_snapshot_directory = "/home/jiajunh/kfold_2"
   5:     max_features = 20
   6:     # model = get_cnn_model(num_amino_acids, 2000, 1, target_function)
   7:     #target_function='bp'
   8:     #func_df=pd.read_pickle('../data/'+str(target_function)+'_all.pkl')
   9:     #functions=func_df['functions'].values
  10:     output_node=9
  11: 
  12:     input_embed = Input(shape=(800,), name='input_embed')
  13:     input_extra = Input(shape=(800, 20,), name='input_extra')
  14:     embedded = Embedding(max_features, 128, input_length=800)(input_embed)
  15:     x1 = concatenate([embedded, input_extra], axis=2)
  16:     x1 = Conv1D(filters=64, kernel_size=3,
  17:                 activation='relu', strides=1,name='conv1')(x1)
  18:     x1 = Dropout(0.21716,name='drop1')(x1)
  19:     x1 = Conv1D(filters=128, kernel_size=7,
  20:                 activation='relu', strides=1,name='conv2')(x1)
  21:     x1 = Dropout(0.04161,name='drop2')(x1)
  22:     x1 = MaxPooling1D(pool_size=8, strides=16,name='maxpooling')(x1)
  23:     x1 = BatchNormalization()(x1)
  24: 
  25:     x1_left = GRU(units=100, recurrent_activation='hard_sigmoid',
  26:              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',
  27:              activation='tanh', return_sequences=True, dropout=0.97710,name='gru1')(x1)
  28:     x1_right = GRU(units=200, recurrent_activation='hard_sigmoid',
  29:              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',
  30:              activation='tanh', return_sequences=True, dropout=0.67130,name='gru2',
  31:              go_backwards='True')(x1)
  32:     xgru1 = keras.layers.concatenate([x1_left, x1_right], axis=-1)
  33: 
  34:     x2_left=GRU(units=400, recurrent_activation='hard_sigmoid',
  35:              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',
  36:              activation='tanh', return_sequences=True, dropout=0.04987,name='gru3')(xgru1)
  37:     x2_right=GRU(units=200, recurrent_activation='hard_sigmoid',
  38:              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',
  39:              activation='tanh', return_sequences=True, dropout=0.66720,name='gru4',
  40:            go_backwards='True')(xgru1)
  41:     x_gru=keras.layers.concatenate([x2_left,x2_right],axis=-1)
  42: 
  43:     x = BatchNormalization()(x_gru)
  44:     x = Flatten()(x)
  45:     x = Dense(units=256, activation='relu',
  46:               kernel_regularizer=regularizers.l1_l2(0.00001),name='dense1')(x)
  47:     x = BatchNormalization()(x)
  48:     output = Dense(output_node, activation='softmax')(x)
  49:     model = Model(inputs=[input_embed, input_extra], outputs=output)
  50:     print(model.summary())
  51: 
  52:     loss_fn = 'categorical_crossentropy'
  53:     adam = keras.optimizers.Adam(lr=10 ** -3,clipnorm=1.)
  54:     optim = AdaBound(lr=1e-03, final_lr=0.1, gamma=1e-03, weight_decay=0, amsbound=False)
  55: 
  56:     filepath = model_snapshot_directory + '/'+ '_weights.hdf5'
  57:     gpu_model = multi_gpu_model(model, 2)
  58:     # model.compile(loss=loss_fn, optimizer=adam, metrics=['accuracy'])
  59:     gpu_model.compile(loss=loss_fn, optimizer=optim, metrics=['accuracy'])
  60:     early_stopping = EarlyStopping(monitor='val_loss', patience=20, mode='min')
  61:     checkpointer = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1,
  62:                                    save_best_only=True, save_weights_only=True, mode='min')
  63:     callback_list = [early_stopping, checkpointer]
  64:     hist = gpu_model.fit(x={'input_embed': X_train, 'input_extra': X_extra},
  65:                          y=y_train,
  66:                          epochs=150, batch_size=64,
  67:                          verbose=1, callbacks=callback_list,
  68:                          validation_split=0.2)
  69:     score, acc =gpu_model.evaluate(x={'input_embed':X_train,'input_extra':X_extra},y=y_train)
  70:     h1 = hist.history
  71:     epoch=hist.epoch
  72:     acc_ = np.asarray(h1['acc'])
  73:     loss_ = np.asarray((h1['loss']))
  74:     val_acc = np.asarray(h1['val_acc'])
  75:     val_loss = np.asarray(h1['val_loss'])
  76:     acc_and_loss = np.column_stack((epoch,acc_, loss_, val_acc, val_loss))
  77:     save_file_mlp = model_snapshot_directory + '/mlp_run_1' + '.txt'
  78:     with open(save_file_mlp, 'w') as f:
  79:         np.savetxt(save_file_mlp, acc_and_loss, delimiter=" ")
  80:     print('train accuracy:', acc)
  81:     print('train score:',score)
  82:     print("Test:------------------------------------------------------------")
  83:     score_test,acc_test = gpu_model.evaluate(x={'input_embed': X_test, 'input_extra': pssm_ssacc_test}, y=y_test)
  84:     y_preds=gpu_model.predict(x={'input_embed':X_test,'input_extra':pssm_ssacc_test})
  85:     print('test accuracy:', acc_test)
  86:     print('test score:',score_test)
  87:     # get the best threshold according to mcc value
  88:     return {'loss': -acc, 'status': STATUS_OK, 'model': gpu_model}
  89: 
