>>> Imports:
#coding=utf-8

from __future__ import print_function

try:
    import numpy as np
except:
    pass

try:
    from utils import *
except:
    pass

try:
    import math
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    from keras.backend.tensorflow_backend import set_session
except:
    pass

try:
    from keras.models import Sequential, Model
except:
    pass

try:
    from keras.layers import Dense, Dropout, Activation, GRU
except:
    pass

try:
    from keras.layers import Embedding, concatenate, Flatten
except:
    pass

try:
    from keras.layers import Conv1D
except:
    pass

try:
    from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D, MaxPooling1D
except:
    pass

try:
    from keras.layers import Input
except:
    pass

try:
    from keras.layers.normalization import BatchNormalization
except:
    pass

try:
    import logging
except:
    pass

try:
    import json
except:
    pass

try:
    from keras.preprocessing import sequence
except:
    pass

try:
    from keras.callbacks import EarlyStopping, ModelCheckpoint
except:
    pass

try:
    from hyperas.distributions import uniform, choice
except:
    pass

try:
    from hyperopt import Trials, STATUS_OK, tpe
except:
    pass

try:
    from hyperas import optim
except:
    pass

try:
    import keras
except:
    pass

try:
    from keras import regularizers
except:
    pass

try:
    from keras.utils import multi_gpu_model
except:
    pass

try:
    from tensorflow.contrib.metrics import streaming_auc
except:
    pass

try:
    from sklearn.metrics import matthews_corrcoef, recall_score, f1_score
except:
    pass

try:
    from my_utils import *
except:
    pass

try:
    import os
except:
    pass

try:
    import tensorflow as tf
except:
    pass

try:
    from keras.backend.tensorflow_backend import set_session
except:
    pass

try:
    from adabound import AdaBound
except:
    pass

try:
    from sklearn.preprocessing import LabelBinarizer
except:
    pass

try:
    from keras.utils import to_categorical
except:
    pass

try:
    import sys
except:
    pass

>>> Hyperas search space:

def get_space():
    return {
    }

>>> Data
   1: 
   2: max_sequence_size=800
   3: logging.debug("Target Gene Ontology :BP ")
   4: train_file = '/home/jiajunh/kfold_1/all_train.pkl'
   5: pssm_file = '/home/wanglei/data/Oxygen/npy'
   6: test_file = '/home/jiajunh/kfold_1/all_test.pkl'
   7: pssm_test_file = '/home/wanglei/data/Oxygen/npy'
   8: logging.info("Preparing train data ... ")
   9: 
  10: features = []
  11: train_df=pd.read_pickle(train_file)
  12: 
  13: def reshape(values):
  14:     values=np.hstack(values).reshape(len(values),len(values[0]))
  15:     return values
  16: 
  17: for i,row in train_df.iterrows():
  18:     pdb_id=row['pdb_id']
  19:     extra_feature=get_extra_feature(pssm_file,pdb_id)
  20:     features.append(extra_feature)
  21: 
  22: logging.info('Padding sequences to %d ... ' % max_sequence_size)
  23: X=sequence.pad_sequences(train_df['ngrams'].values,maxlen=max_sequence_size)
  24: y=train_df['labels'].values
  25: lb = LabelBinarizer()
  26: 
  27: y = lb.fit_transform(y)  # transfer label to binary value
  28: 
  29: #y = to_categorical(y)  # transfer binary label to one-hot. IMPORTANT
  30: X_all=np.array(X)
  31: y_all=np.array(y)
  32: pssm_ssacc=np.array(features)
  33: logging.info("Input shape: %s" % str(X_all.shape))
  34: logging.info("Output shape: %s" % str(y_all.shape))
  35: logging.info("pssm and ssacc shape:%s" % str(pssm_ssacc.shape))
  36: n = X_all.shape[0]
  37: 
  38: # randomize to shuffle first
  39: randomize = np.arange(n)
  40: np.random.shuffle(randomize)
  41: 
  42: X_train = X_all[randomize]
  43: y_train_temp = y_all[randomize]
  44: y_train=reshape(y_train_temp)
  45: X_extra = pssm_ssacc[randomize]
  46: 
  47: logging.info('------------------------------------------------')
  48: logging.info('preparing the test data...')
  49: 
  50: features_test = []
  51: test_df = pd.read_pickle(test_file)
  52: for i,row in test_df.iterrows():
  53:     pdb_id = row['pdb_id']
  54:     extra_feature = get_extra_feature(pssm_test_file, pdb_id)
  55:     features_test.append(extra_feature)
  56: logging.info('Padding sequences to %d ... ' % max_sequence_size)
  57: X_temp = sequence.pad_sequences(test_df['ngrams'].values, maxlen=max_sequence_size)
  58: y_temp = test_df['labels'].values
  59: X_test=np.array(X_temp)
  60: y_test_temp=np.array(y_temp)
  61: y_test=reshape(y_test_temp)
  62: pssm_ssacc_test=np.array(features_test)
  63: logging.info("Input shape: %s" % str(X_test.shape))
  64: logging.info("Output shape: %s" % str(y_test.shape))
  65: logging.info("pssm and ssacc shape:%s" % str(pssm_ssacc_test.shape))
  66: 
  67: 
  68: 
>>> Resulting replaced keras model:

   1: def keras_fmin_fnct(space):
   2: 
   3:     test_file = '/home/jiajunh/kfold_1/all_test.pkl'
   4:     test_df = pd.read_pickle(test_file)
   5:     y_true=test_df['ngrams'].values
   6:     # num_amino_acids = 22  # alphabet of proteins +1 for padding
   7:     model_snapshot_directory = "/home/jiajunh/kfold_1"
   8:     max_features = 20
   9:     # model = get_cnn_model(num_amino_acids, 2000, 1, target_function)
  10:     #target_function='bp'
  11:     #func_df=pd.read_pickle('../data/'+str(target_function)+'_all.pkl')
  12:     #functions=func_df['functions'].values
  13:     output_node=9
  14: 
  15:     input_embed = Input(shape=(800,), name='input_embed')
  16:     input_extra = Input(shape=(800, 20,), name='input_extra')
  17:     embedded = Embedding(max_features, 128, input_length=800)(input_embed)
  18:     x1 = concatenate([embedded, input_extra], axis=2)
  19:     x1 = Conv1D(filters=64, kernel_size=11,
  20:                 activation='relu', strides=1,name='conv1')(x1)
  21:     x1 = Dropout(0.02007,name='drop1')(x1)
  22:     x1 = Conv1D(filters=128, kernel_size=21,
  23:                 activation='relu', strides=1,name='conv2')(x1)
  24:     x1 = Dropout(0.01850,name='drop2')(x1)
  25:     x1 = MaxPooling1D(pool_size=16, strides=32,name='maxpooling')(x1)
  26:     x1 = BatchNormalization()(x1)
  27: 
  28:     x1_left = GRU(units=200, recurrent_activation='hard_sigmoid',
  29:              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',
  30:              activation='tanh', return_sequences=True, dropout=0.22438,name='gru1')(x1)
  31:     x1_right = GRU(units=300, recurrent_activation='hard_sigmoid',
  32:              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',
  33:              activation='tanh', return_sequences=True, dropout=0.54717,name='gru2',
  34:              go_backwards='True')(x1)
  35:     xgru1 = keras.layers.concatenate([x1_left, x1_right], axis=-1)
  36: 
  37:     x2_left=GRU(units=200, recurrent_activation='hard_sigmoid',
  38:              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',
  39:              activation='tanh', return_sequences=True, dropout=0.64088,name='gru3')(xgru1)
  40:     x2_right=GRU(units=200, recurrent_activation='hard_sigmoid',
  41:              use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros',
  42:              activation='tanh', return_sequences=True, dropout=0.00015,name='gru4',
  43:            go_backwards='True')(xgru1)
  44:     x_gru=keras.layers.concatenate([x2_left,x2_right],axis=-1)
  45: 
  46:     x = BatchNormalization()(x_gru)
  47:     x = Flatten()(x)
  48:     x = Dense(units=64, activation='relu',
  49:               kernel_regularizer=regularizers.l1_l2(0.0001),name='dense1')(x)
  50:     x = BatchNormalization()(x)
  51:     output = Dense(output_node, activation='softmax')(x)
  52:     model = Model(inputs=[input_embed, input_extra], outputs=output)
  53:     print(model.summary())
  54: 
  55:     loss_fn = 'categorical_crossentropy'
  56:     adam = keras.optimizers.Adam(lr=10 ** -3,clipnorm=1.)
  57:     optim = AdaBound(lr=1e-03, final_lr=0.1, gamma=1e-03, weight_decay=0, amsbound=False)
  58: 
  59:     filepath = model_snapshot_directory + '/'+ '_weights.hdf5'
  60:     gpu_model = multi_gpu_model(model, 2)
  61:     # model.compile(loss=loss_fn, optimizer=adam, metrics=['accuracy'])
  62:     gpu_model.compile(loss=loss_fn, optimizer=optim, metrics=['accuracy'])
  63:     early_stopping = EarlyStopping(monitor='val_loss', patience=20, mode='min')
  64:     checkpointer = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1,
  65:                                    save_best_only=True, save_weights_only=True, mode='min')
  66:     callback_list = [early_stopping, checkpointer]
  67:     hist = gpu_model.fit(x={'input_embed': X_train, 'input_extra': X_extra},
  68:                          y=y_train,
  69:                          epochs=150, batch_size=64,
  70:                          verbose=1, callbacks=callback_list,
  71:                          validation_split=0.2)
  72:     score, acc =gpu_model.evaluate(x={'input_embed':X_train,'input_extra':X_extra},y=y_train)
  73:     h1 = hist.history
  74:     epoch=hist.epoch
  75:     acc_ = np.asarray(h1['acc'])
  76:     loss_ = np.asarray((h1['loss']))
  77:     val_acc = np.asarray(h1['val_acc'])
  78:     val_loss = np.asarray(h1['val_loss'])
  79:     acc_and_loss = np.column_stack((epoch,acc_, loss_, val_acc, val_loss))
  80:     #save_file_mlp = model_snapshot_directory + '/mlp_run_' + '.txt'
  81:     #with open(save_file_mlp, 'w') as f:
  82:     #    np.savetxt(save_file_mlp, acc_and_loss, delimiter=" ")
  83:     print('train accuracy:', acc)
  84:     print('train score:',score)
  85:     print("Test:------------------------------------------------------------")
  86:     score_test,acc_test = gpu_model.evaluate(x={'input_embed': X_test, 'input_extra': pssm_ssacc_test}, y=y_test)
  87:     print('test accuracy:', acc_test)
  88:     print('test score:',score_test)
  89:     y_pred=gpu_model.predict(x={'input_embed':X_test,'input_extra':pssm_ssacc_test})
  90:     save_file=model_snapshot_directory+'/'
  91:     fig_name=ROC
  92:     auc_value, thre=get_auc_bestthre(y_true,y_pred,save_file,fig_name)
  93:     print('auc:',auc_value)
  94:     TP, TN, FN, FP=calc_confusion_matrix(y_true, y_pred, threshold=None)
  95:     recall=calc_recall(TP,TN,FN,FP)
  96:     precision=calc_precision(TP,TN,FN,FP)
  97:     print('recall:',recall)
  98:     print('precision:',precision)
  99:     recall_=np.asarray(recall)
 100:     precision_=np.asarray(precision)
 101:     precision_and_recall = np.column_stack((precision,recall))
 102:     save_file_pr = model_snapshot_directory + '/p_r' + '.txt'
 103:     with open(save_file_pr, 'w') as f:
 104:         np.savetxt(save_file_pr, precision_and_recall, delimiter=" ")    # get the best threshold according to mcc value
 105:     return {'loss': -acc, 'status': STATUS_OK, 'model': gpu_model}
 106: 
